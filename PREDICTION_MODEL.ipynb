{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0hP5p9uYXkQ"
      },
      "outputs": [],
      "source": [
        "!pip install lightgbm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IyYHriOdYbjz"
      },
      "outputs": [],
      "source": [
        "!pip install imblearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tmul4PmaboZF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "from imblearn.combine import SMOTEENN\n",
        "import joblib\n",
        "\n",
        "# 1. Load Data from Google Drive\n",
        "print(\" Step 1: Loading Data \")\n",
        "# This block is for mounting your Google Drive in the Colab environment.\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    #Download the csv file  from the drive link(provided in DATA.txt) to your google drive and paste the path of the file instead of this path.\n",
        "    file_path = '/content/drive/MyDrive/fires_cleaned_final.csv'\n",
        "    df = pd.read_csv(file_path, low_memory=False)\n",
        "    print(\" CSV Data loaded successfully from Google Drive.\")\n",
        "except Exception as e:\n",
        "    print(f\" An error occurred while loading the data from Google Drive: {e}\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# 2. Clean and Filter Data\n",
        "print(\"\\n Cleaning and Filtering Data \")\n",
        "initial_rows = len(df)\n",
        "vague_causes = ['Missing/Undefined', 'Miscellaneous']\n",
        "df_filtered = df[~df['STAT_CAUSE_DESCR'].isin(vague_causes)]\n",
        "print(f\"Removed {initial_rows - len(df_filtered)} rows with vague causes.\")\n",
        "print(f\"Remaining classes for training: {df_filtered['STAT_CAUSE_DESCR'].unique()}\")\n",
        "\n",
        "\n",
        "# 3. Advanced Feature Engineering\n",
        "print(\"\\n Advanced Feature Engineering \")\n",
        "features = ['FIRE_YEAR', 'DISCOVERY_DOY', 'FIRE_SIZE', 'LATITUDE', 'LONGITUDE',\n",
        "            'OWNER_CODE', 'STATE', 'NWCG_REPORTING_AGENCY']\n",
        "target = 'STAT_CAUSE_DESCR'\n",
        "df_model = df_filtered[features + [target]].copy()\n",
        "df_model.dropna(inplace=True)\n",
        "\n",
        "# Create cyclical time features\n",
        "df_model['doy_sin'] = np.sin(2 * np.pi * df_model['DISCOVERY_DOY']/365.0)\n",
        "df_model['doy_cos'] = np.cos(2 * np.pi * df_model['DISCOVERY_DOY']/365.0)\n",
        "\n",
        "# Create interaction feature\n",
        "df_model['lat_lon_interaction'] = df_model['LATITUDE'] * df_model['LONGITUDE']\n",
        "\n",
        "# Prepare final feature set\n",
        "X = df_model.drop([target, 'DISCOVERY_DOY'], axis=1)\n",
        "y = df_model[target]\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Convert categorical columns to integer codes for SMOTEENN\n",
        "categorical_features = ['OWNER_CODE', 'STATE', 'NWCG_REPORTING_AGENCY']\n",
        "category_maps = {}\n",
        "for col in categorical_features:\n",
        "    X[col] = X[col].astype('category')\n",
        "    category_maps[col] = dict(zip(X[col].cat.categories, X[col].cat.codes))\n",
        "    X[col] = X[col].cat.codes\n",
        "print(\" Advanced feature engineering complete. Categorical features are now integer encoded.\")\n",
        "\n",
        "# 4. Split Data\n",
        "print(\"\\n Splitting Data \")\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n",
        "print(\" Data splitting complete.\")\n",
        "\n",
        "#5. Apply SMOTEENN\n",
        "print(\"\\n Applying SMOTEENN \")\n",
        "sme = SMOTEENN(random_state=42)\n",
        "X_train_resampled, y_train_resampled = sme.fit_resample(X_train, y_train)\n",
        "print(f\" SMOTEENN complete. New training set size: {len(X_train_resampled)}\")\n",
        "\n",
        "#6. Train a More Aggressive LightGBM Model\n",
        "print(\"\\n Training High-Performance Model  \")\n",
        "\n",
        "lgbm_tuned = lgb.LGBMClassifier(\n",
        "    objective='multiclass',\n",
        "    n_jobs=-1,\n",
        "    num_class=len(label_encoder.classes_),\n",
        "    n_estimators=2000,\n",
        "    learning_rate=0.02, num_leaves=61, max_depth=15,\n",
        "    reg_alpha=0.1, reg_lambda=0.1, random_state=42,\n",
        "    colsample_bytree=0.8, subsample=0.8\n",
        ")\n",
        "# Tell LightGBM which integer columns should be treated as categories\n",
        "lgbm_tuned.fit(X_train_resampled, y_train_resampled, eval_set=[(X_test, y_test)],\n",
        "               eval_metric='multi_logloss', callbacks=[lgb.early_stopping(150, verbose=False)],\n",
        "               categorical_feature=categorical_features)\n",
        "print(\" Model training complete.\")\n",
        "\n",
        "#7. Evaluate and Save Artifacts\n",
        "print(\"\\n Evaluating and Saving Model \")\n",
        "y_pred = lgbm_tuned.predict(X_test)\n",
        "print(\"\\nFinal Classification Report (Focused Model):\")\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
        "\n",
        "# Save the model and artifacts to your Google Drive for later use\n",
        "output_path = '/content/drive/MyDrive/'\n",
        "joblib.dump(lgbm_tuned, output_path + 'wildfire_cause_model_focused.joblib')\n",
        "joblib.dump(label_encoder, output_path + 'label_encoder_focused.joblib')\n",
        "joblib.dump(X.columns.tolist(), output_path + 'training_columns_focused.joblib')\n",
        "joblib.dump(category_maps, output_path + 'category_maps_focused.joblib')\n",
        "#Added location data back to the defaults\n",
        "default_values = {\n",
        "    'FIRE_YEAR': X['FIRE_YEAR'].mode()[0], 'FIRE_SIZE': X['FIRE_SIZE'].median(),\n",
        "    'DISCOVERY_DOY': 182,\n",
        "    'LATITUDE': 39.8283, # Center of the US\n",
        "    'LONGITUDE': -98.5795, # Center of the US\n",
        "    'OWNER_CODE': X['OWNER_CODE'].mode()[0], 'STATE': X['STATE'].mode()[0],\n",
        "    'NWCG_REPORTING_AGENCY': X['NWCG_REPORTING_AGENCY'].mode()[0]\n",
        "}\n",
        "joblib.dump(default_values, output_path + 'default_values_focused.joblib')\n",
        "print(f\"\\n Focused model and artifacts have been saved successfully to your Google Drive at: {output_path}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
